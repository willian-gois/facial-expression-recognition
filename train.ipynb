{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo para Reconhecimento de Express√µes Faciais"
      ],
      "metadata": {
        "id": "LqWuYwqEI9gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.optimizers import *\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "6Ar2QfNQFkP5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar dataset\n",
        "\n",
        "# Carregando do meu drive pessoal devido ao tamanho do dataset\n",
        "# Mas o arquivo original pode ser encontrado aqui: https://www.kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge/overview\n",
        "filename = '/content/drive/MyDrive/IFRS/fer2013.csv'\n",
        "columns_names = ['emotion', 'pixels', 'usage']\n",
        "df = pd.read_csv(filename, names=columns_names, na_filter=False)\n",
        "\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "40Kx8SB3Fnww",
        "outputId": "52da32c7-0578-44d1-d3de-76b67c6e9fd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   emotion                                             pixels     usage\n",
              "0  emotion                                             pixels     Usage\n",
              "1        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "2        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e68f6c55-92b0-43f0-a02c-6c7f67b2a510\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>emotion</td>\n",
              "      <td>pixels</td>\n",
              "      <td>Usage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e68f6c55-92b0-43f0-a02c-6c7f67b2a510')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e68f6c55-92b0-43f0-a02c-6c7f67b2a510 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e68f6c55-92b0-43f0-a02c-6c7f67b2a510');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1be62779-0c59-470f-a071-0e8e589ab7f1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1be62779-0c59-470f-a071-0e8e589ab7f1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1be62779-0c59-470f-a071-0e8e589ab7f1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 35888,\n  \"fields\": [\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"0\",\n          \"3\",\n          \"emotion\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pixels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34035,\n        \"samples\": [\n          \"54 17 1 10 12 15 29 34 31 25 22 23 19 15 18 14 10 11 10 12 16 15 14 12 15 12 12 11 11 11 11 12 26 40 55 68 49 39 51 37 25 19 2 23 141 135 191 193 50 5 5 7 10 20 31 29 27 25 27 28 20 19 22 18 14 15 12 13 18 18 17 15 20 15 10 16 11 13 15 11 21 38 73 89 62 46 41 39 28 21 8 0 101 134 152 202 31 0 6 5 8 21 27 28 24 25 29 33 24 26 26 21 17 19 15 15 20 19 19 20 27 22 9 21 13 17 20 15 20 40 82 95 78 60 38 31 33 21 10 0 47 147 96 183 10 3 6 5 8 21 23 27 22 27 38 33 28 35 30 24 19 26 20 15 20 23 22 25 33 28 14 21 23 21 27 26 21 44 80 106 105 75 47 32 25 25 15 5 2 116 101 136 3 6 5 5 9 16 22 21 22 32 41 29 37 38 32 28 25 32 23 19 24 27 24 21 34 46 22 25 30 32 35 38 32 46 79 108 132 88 54 45 29 25 16 4 49 120 98 79 4 6 5 4 9 14 21 17 24 38 36 34 43 35 33 29 30 37 27 22 25 27 25 24 33 71 34 31 35 45 48 49 44 46 85 116 139 105 54 54 44 34 20 3 63 87 59 62 6 6 4 5 9 13 15 15 29 46 33 43 39 39 38 36 36 42 30 29 26 34 31 27 30 91 63 38 49 53 61 58 55 54 70 115 145 126 57 52 47 43 19 10 15 33 77 62 5 7 4 7 8 11 11 16 32 36 33 47 42 42 42 46 43 44 33 34 28 44 54 31 27 103 121 60 53 62 64 70 62 54 60 99 145 141 78 48 40 41 14 3 61 79 121 53 4 6 4 8 5 10 9 21 34 23 37 47 41 47 45 46 50 50 40 38 35 49 67 43 32 87 162 108 64 60 67 67 64 57 57 74 138 146 90 51 39 35 22 1 17 60 99 55 4 5 4 7 5 8 12 21 25 17 41 43 42 43 43 48 57 56 44 46 37 55 81 56 40 64 164 156 102 59 61 63 56 53 52 50 105 169 107 58 51 31 21 5 20 27 24 24 5 4 6 6 6 5 17 21 15 18 43 35 38 38 47 49 67 59 48 51 41 57 85 70 50 50 137 194 168 98 48 53 47 45 42 41 42 139 150 61 60 40 34 6 38 41 54 49 6 3 8 5 7 7 20 13 8 23 32 26 31 42 44 50 82 58 57 52 47 62 92 99 54 53 96 159 115 89 66 49 39 33 29 37 25 75 168 79 49 51 35 7 44 49 62 109 4 3 8 7 7 9 17 8 8 25 24 19 24 29 32 74 97 53 54 57 44 65 95 138 67 51 59 115 126 157 188 173 149 140 95 81 68 32 134 115 41 55 49 12 38 48 61 128 14 1 7 8 8 12 17 7 9 22 18 15 16 16 24 59 62 45 54 57 44 69 105 174 137 52 37 99 204 208 214 223 230 230 219 197 191 139 116 141 52 55 56 12 52 61 67 142 8 6 8 9 7 19 19 8 11 13 18 16 21 15 48 89 53 46 77 65 83 136 186 218 205 110 67 144 199 191 193 192 199 202 207 215 213 215 190 157 68 49 54 14 86 83 78 151 24 13 11 16 7 18 16 9 10 5 27 18 53 35 130 169 118 120 124 147 193 217 228 231 225 204 187 203 168 128 107 101 104 118 160 196 211 210 208 185 87 44 53 22 109 114 103 156 47 34 8 24 9 18 13 12 13 64 97 92 165 166 189 181 163 177 186 195 203 212 226 228 222 217 207 173 96 24 93 208 170 110 61 146 198 211 208 203 117 38 50 31 124 136 126 163 62 65 4 25 15 28 33 58 120 161 176 175 148 128 134 156 179 182 182 186 190 206 225 230 220 212 203 149 33 30 156 252 239 158 98 165 197 212 210 205 145 38 43 40 128 148 144 165 60 94 18 24 18 39 96 133 159 161 137 63 36 31 101 154 142 173 190 177 175 194 218 226 216 208 212 134 49 46 96 180 149 138 166 193 206 211 212 208 166 39 38 50 121 144 137 140 53 96 37 17 23 43 117 144 157 138 71 40 35 20 150 255 237 147 178 179 165 189 211 220 212 205 211 184 156 146 153 170 195 209 207 209 213 214 212 210 176 40 36 52 97 116 103 94 54 97 56 14 25 40 121 150 159 137 94 82 55 29 132 211 179 135 170 181 166 186 206 213 211 204 204 210 204 195 203 216 219 217 217 216 218 217 214 211 183 36 35 52 79 108 77 78 73 99 89 13 30 34 121 153 162 160 151 117 102 96 123 149 180 208 199 175 168 188 206 213 211 207 206 211 214 207 204 208 212 215 219 222 222 221 217 215 181 30 34 58 68 119 140 81 99 112 116 14 30 28 119 152 162 167 170 170 170 183 190 197 207 208 194 178 172 186 203 212 213 210 213 217 220 221 217 214 218 221 223 225 226 221 217 216 163 20 34 55 56 77 65 17 116 125 133 23 32 20 107 147 159 173 180 182 192 196 197 201 208 206 195 181 174 183 200 209 213 211 213 218 223 224 223 223 223 224 228 228 226 220 217 214 141 15 38 57 57 47 12 13 112 113 117 38 32 13 81 145 157 174 186 191 195 199 205 208 210 205 192 178 171 178 200 208 211 209 211 216 221 224 223 222 224 228 231 228 224 221 215 212 111 16 38 51 55 39 21 8 73 73 61 34 37 13 52 135 154 174 187 197 203 206 211 213 211 204 190 174 167 171 199 207 209 209 210 215 222 224 224 223 225 229 228 226 224 220 214 208 87 17 39 40 58 34 23 13 64 54 46 37 35 19 21 116 150 170 184 196 202 208 210 213 213 206 193 173 158 171 200 206 212 214 210 208 219 222 223 226 228 230 228 225 223 221 214 202 68 22 40 39 59 33 12 19 44 83 57 37 40 21 6 78 145 160 177 189 199 206 210 214 214 210 195 166 155 175 203 214 221 219 214 209 214 216 220 227 230 230 227 224 221 219 214 192 45 35 42 37 53 31 18 24 4 35 23 37 42 26 6 37 128 152 171 182 193 203 209 214 216 215 198 157 161 177 202 223 229 223 224 219 214 212 217 224 229 229 226 223 221 217 214 181 28 34 47 33 47 39 22 32 5 8 13 37 43 29 12 14 97 145 162 175 186 200 209 215 219 222 205 163 175 182 200 217 223 209 202 218 214 213 216 222 228 227 224 223 221 217 213 163 12 30 49 31 43 48 30 37 7 14 15 37 39 32 15 11 58 136 151 166 178 193 205 214 220 223 217 188 189 185 168 201 214 206 197 207 208 211 216 222 226 225 225 223 221 215 215 139 2 37 56 29 38 43 39 33 4 8 12 31 36 31 15 22 57 106 150 158 172 187 200 210 217 223 217 205 196 186 172 190 202 201 200 201 206 213 217 221 225 226 225 223 221 215 213 108 0 39 80 22 31 43 37 21 3 6 13 29 32 32 20 17 84 70 142 154 169 183 196 206 212 216 211 206 201 192 187 199 201 201 207 205 211 216 217 220 223 225 224 221 220 216 203 65 1 43 89 20 30 42 46 15 5 8 20 26 27 31 25 5 70 53 116 151 167 180 191 201 206 208 208 206 201 190 183 200 199 204 211 212 213 218 219 220 221 221 221 221 218 216 178 24 6 47 88 14 32 43 50 21 8 13 14 19 23 31 26 13 20 24 73 141 159 176 187 195 200 202 204 201 192 179 182 188 195 206 203 208 218 225 222 219 220 220 220 220 213 213 119 5 12 57 83 13 32 45 53 28 16 14 8 15 23 27 26 18 7 51 35 116 150 167 179 188 192 195 196 195 185 181 179 160 177 183 181 178 182 187 190 212 217 220 221 217 209 194 46 10 13 65 69 13 34 51 51 23 22 9 11 18 23 23 26 22 2 65 47 59 137 154 170 179 183 185 185 175 146 125 96 118 171 170 151 163 159 165 190 213 219 220 219 213 211 124 3 19 15 61 43 18 42 54 42 14 7 23 17 16 22 20 20 27 11 22 82 12 93 136 152 165 171 174 167 141 102 89 86 151 190 189 179 185 186 202 216 216 218 218 214 209 187 34 8 19 21 60 33 26 47 51 33 8 80 146 97 22 24 26 18 24 24 3 43 16 26 106 133 147 160 161 149 137 129 100 106 168 189 197 181 188 201 214 217 220 218 215 208 205 79 23 13 5 43 60 28 34 47 47 29 55 165 190 136 110 83 52 34 22 26 17 12 22 0 41 104 124 139 143 138 135 125 108 123 170 193 202 195 207 216 218 221 222 219 210 213 107 0 36 49 42 59 39 35 34 44 36 23 115 166 167 155 155 162 93 48 29 19 21 11 17 12 0 43 100 116 125 133 130 128 135 144 173 197 199 210 218 220 221 222 221 215 218 122 5 12 9 44 54 29 39 35 35 40 29 20 43 175 193 202 186 180 129 33 28 24 22 18 18 13 2 2 36 91 113 122 127 130 139 141 170 197 200 212 222 223 222 223 214 220 139 5 10 8 11 8 9 26 32 30 27 36 48 33 74 203 211 211 202 177 122 91 53 28 17 21 20 21 8 5 4 25 76 111 123 127 138 147 179 204 208 208 217 221 221 214 216 156 15 6 8 6 11 14 18 20 23 15 22 23 18 11 144 198 197 196 203 197 199 162 46 26 25 15 14 17 20 10 6 5 11 52 96 121 131 148 181 204 209 204 211 216 214 209 169 58 10 8 9 10 12 15 16 19 14 15 16 9 10 5 122 201 208 213 212 208 200 192 151 64 28 16 13 8 15 22 12 8 6 5 27 74 116 151 185 205 211 199 205 205 199 185 117 51 17 12 14 13 19 16 31 21 12 19 12 5 10 10 82 209 210 210 209 208 203 189 178 114 30 36 46 41 12 10 18 13 9 10 17 16 67 156 181 202 213 192 189 194 191 170 96 36 15 16 16 22 21 22 34 10 12 7 9 13 14 12 38 210 211 214 214 212 210 204 188 167 143 114 137 123 54 9 13 14 12 10 25 28 34 141 173 194 209 192 189 193 188 155 77 21 15 17 17 20 17 28 25 5 5 9 28 31 15 21 87 218 217 211 211 215 216 213 205 194 179 155 142 169 106 16 13 11 12 13 21 27 38 133 174 193 214 198 187 188 180 140 56 22 21 16 16 17 15 33 14 1 11 37 51 38 13 36 164\",\n          \"60 53 127 234 234 226 216 222 226 223 225 223 221 222 216 209 210 159 99 105 124 150 172 175 174 179 187 184 175 163 144 136 173 194 175 129 75 57 49 55 66 105 143 149 131 173 159 95 61 50 134 241 231 225 227 225 222 221 224 226 228 223 218 209 182 146 133 145 166 183 184 192 195 195 196 192 182 175 172 168 178 195 189 173 119 74 68 65 50 53 101 137 113 136 178 137 60 54 146 238 229 228 216 207 211 214 218 225 229 227 218 199 173 166 160 167 178 181 185 194 192 184 176 178 171 159 161 171 192 210 200 189 172 123 82 60 69 86 110 133 143 131 164 137 60 62 149 236 225 177 133 127 165 196 205 215 225 228 213 187 186 171 164 164 163 170 171 166 148 145 148 148 154 159 148 153 177 217 212 196 187 154 90 50 45 80 96 118 178 172 156 135 60 60 150 221 151 106 123 135 147 181 194 202 217 223 204 188 190 171 151 147 153 156 143 131 172 196 189 170 149 151 152 143 153 201 225 200 191 179 133 74 53 78 85 91 112 168 176 166 58 66 152 157 114 160 198 187 175 183 194 202 206 207 197 192 183 156 134 139 135 136 125 183 189 153 147 144 155 153 146 144 140 161 218 206 189 190 170 107 58 65 78 97 108 138 164 171 59 59 144 184 203 242 234 223 214 203 197 200 210 195 194 193 173 141 117 121 124 120 172 154 83 57 77 113 84 96 117 129 135 146 188 207 191 198 184 147 70 49 93 82 64 92 165 108 61 59 139 209 242 242 231 221 218 214 208 208 210 206 200 192 164 114 105 99 115 153 173 110 127 97 112 172 117 84 105 120 132 145 166 195 185 190 188 168 113 62 59 121 79 119 133 104 61 58 112 213 244 234 219 200 194 198 197 200 211 222 207 195 152 90 94 79 127 160 151 208 137 125 162 154 140 150 149 130 147 153 160 183 182 178 187 177 132 83 108 152 95 133 90 135 63 59 113 227 239 209 179 175 182 180 177 172 185 227 230 200 148 88 96 81 141 151 178 196 193 195 190 174 155 131 122 142 159 157 154 175 187 182 188 174 129 100 114 165 115 119 87 147 64 58 94 229 227 176 192 208 194 189 180 172 162 201 237 208 172 116 118 129 157 165 179 186 197 190 176 153 125 124 159 175 168 160 155 167 192 186 192 177 119 100 95 178 136 79 85 125 64 61 63 204 218 177 156 102 91 142 183 178 179 207 233 216 185 143 134 146 155 164 162 162 162 155 147 148 165 186 194 186 172 164 157 164 189 191 198 187 113 69 80 180 161 70 61 106 63 64 53 157 230 149 117 136 121 126 215 193 190 224 228 218 189 157 132 142 152 168 188 191 192 190 191 201 207 207 199 186 169 163 161 167 183 189 204 200 156 50 77 194 152 136 68 126 64 64 55 101 220 137 201 152 131 171 197 187 176 226 227 212 185 160 136 136 153 165 185 201 205 207 208 210 212 211 196 182 172 159 158 168 174 183 209 209 205 158 153 205 136 148 93 111 63 64 61 66 191 168 188 193 200 202 184 170 193 237 229 212 184 160 144 136 138 158 173 194 203 205 206 205 204 201 190 176 164 158 154 165 171 180 209 213 208 202 207 207 159 119 63 112 63 63 64 54 155 227 222 220 205 185 181 202 222 239 232 212 188 165 145 136 135 149 166 179 193 199 199 200 197 193 186 176 164 155 154 159 170 179 208 210 210 202 194 204 154 53 99 150 63 64 64 54 114 240 224 205 191 192 201 219 227 236 235 219 197 181 157 130 127 134 160 178 183 188 188 190 189 187 181 169 160 153 155 158 170 178 202 206 206 204 188 193 111 100 165 187 67 64 64 58 84 223 235 222 214 213 221 228 228 235 234 226 209 193 182 153 139 135 134 163 178 176 174 176 177 177 172 158 148 144 150 157 164 176 195 203 204 206 195 168 91 123 137 158 58 58 64 62 69 201 245 230 224 229 232 227 233 236 232 227 217 195 182 181 170 169 137 119 155 165 164 164 165 161 158 150 137 133 142 150 155 168 190 201 205 210 204 138 82 111 138 153 150 98 60 57 66 173 244 232 232 231 227 230 231 235 230 229 222 190 168 174 181 181 169 95 108 141 155 158 154 151 146 140 132 133 140 142 149 163 183 198 206 216 213 153 126 151 142 130 207 203 165 95 58 146 245 237 233 228 228 227 218 238 232 221 209 167 150 100 81 113 129 71 89 124 147 159 156 146 141 135 135 136 143 145 153 161 176 193 205 221 225 192 204 217 95 111 189 186 198 194 131 105 232 240 233 226 226 220 203 224 216 186 177 147 86 63 57 64 55 57 75 112 145 150 161 154 138 137 139 138 145 156 154 156 170 186 205 224 237 191 218 202 117 115 227 214 204 203 210 174 216 244 231 222 222 212 191 195 175 176 160 148 108 90 70 46 36 43 60 87 107 111 131 155 151 137 141 141 147 153 148 149 160 180 202 224 239 196 224 176 101 94 251 250 249 238 227 202 216 234 228 220 215 205 186 192 186 156 142 120 103 67 40 39 40 44 58 61 73 73 68 97 138 151 140 145 152 151 147 142 151 173 196 221 237 199 228 167 104 86 247 246 249 249 242 224 201 221 223 216 209 200 187 198 176 111 91 49 43 64 41 62 79 84 88 84 84 81 60 54 77 138 147 141 159 153 141 145 146 165 193 219 235 204 230 152 105 87 245 246 246 250 249 251 206 194 219 210 206 198 195 189 147 103 101 95 85 111 138 146 149 145 137 123 117 106 82 59 47 94 148 144 160 157 150 153 148 161 191 219 236 207 226 143 107 105 244 245 244 249 246 244 244 209 207 213 208 203 208 176 123 151 185 194 197 179 157 149 128 110 104 99 95 97 90 78 57 66 135 150 164 166 156 162 156 164 192 218 232 206 221 125 97 104 240 240 237 245 246 245 253 222 204 221 211 207 216 158 153 211 194 169 157 141 132 134 116 129 125 139 117 105 96 78 85 82 127 156 164 176 161 158 163 168 189 218 227 206 212 113 105 85 235 237 228 241 242 244 246 223 209 221 216 218 219 159 174 186 118 137 169 189 187 216 199 201 196 183 175 119 77 8 80 135 134 152 167 179 165 161 162 174 187 218 226 204 209 112 71 41 229 231 224 237 236 237 249 218 199 171 223 216 219 171 171 148 92 208 228 238 221 238 234 204 161 89 95 15 7 14 73 155 131 147 171 177 170 167 168 172 186 215 225 202 209 89 39 44 209 217 220 231 234 237 239 206 219 87 187 225 223 194 166 146 112 234 242 250 237 210 176 76 26 14 10 19 50 40 123 165 132 123 167 179 174 166 175 181 189 217 219 200 206 53 32 37 169 176 182 194 220 224 236 209 219 62 100 223 222 215 169 157 103 157 149 140 116 38 43 66 57 40 45 78 77 83 168 163 154 114 141 182 175 167 176 190 195 218 207 199 211 35 30 29 161 161 155 155 199 224 199 213 188 58 53 154 231 221 185 164 119 27 16 11 9 23 93 120 106 63 96 132 91 141 166 158 169 125 123 181 175 168 175 193 204 220 190 199 232 56 15 33 156 152 145 154 172 191 187 205 127 54 62 71 201 226 205 174 176 101 21 26 18 51 136 114 124 95 139 143 137 161 151 156 169 133 110 157 176 167 177 193 212 218 183 201 249 100 9 40 149 144 141 167 189 162 161 184 75 60 66 55 109 227 217 178 178 203 79 20 26 96 181 142 86 122 177 168 164 139 141 149 161 129 106 134 166 172 171 193 221 211 185 201 253 154 10 32 143 137 149 156 159 165 180 122 57 66 64 66 54 154 223 172 161 203 195 83 76 133 188 180 156 192 199 197 152 118 139 140 157 124 105 128 149 174 177 194 218 196 180 203 248 194 22 33 134 135 156 164 142 169 158 64 64 65 64 65 63 65 181 191 161 189 216 181 128 194 209 215 221 207 201 193 123 114 136 134 150 129 103 128 147 168 181 201 212 174 172 204 241 228 52 32 130 138 141 145 167 171 78 62 68 66 66 66 67 59 92 188 163 175 211 222 200 194 199 196 200 201 204 165 103 122 132 130 147 128 99 116 150 170 186 210 212 157 163 205 236 249 92 27 144 133 143 157 137 75 62 67 67 68 67 67 66 67 56 121 160 158 194 227 217 203 195 197 203 190 169 121 116 129 122 129 155 125 96 102 117 171 190 217 214 157 155 202 233 252 144 63 152 135 139 106 65 64 69 67 68 68 68 68 67 66 63 73 137 157 177 212 210 183 189 183 170 149 132 129 129 116 118 144 168 130 96 89 97 152 201 221 215 166 151 196 235 243 213 101 101 90 70 59 66 68 67 67 68 68 67 68 67 68 66 65 99 140 172 190 216 182 162 153 143 136 134 133 126 126 139 168 167 121 92 79 88 146 201 215 216 175 158 192 234 241 224 164 61 61 65 68 67 67 68 68 68 68 68 68 68 67 67 62 76 116 158 180 202 206 184 163 149 149 144 142 146 155 166 180 157 114 100 95 114 161 195 207 215 182 169 192 232 237 226 113 67 68 68 67 68 68 69 69 69 69 70 70 70 69 68 66 67 91 132 175 187 202 194 174 168 175 171 165 170 173 174 163 128 99 89 98 124 169 187 198 208 190 178 198 233 229 214 63 66 67 67 68 68 68 69 70 70 70 69 70 69 69 68 67 63 73 108 156 187 201 203 199 196 192 182 177 168 161 154 134 109 89 93 115 140 176 179 188 201 187 184 204 233 217 158 38 66 66 67 69 69 69 69 70 70 70 70 70 70 69 68 67 61 78 143 127 174 182 184 192 187 175 163 160 152 161 149 122 85 81 106 127 139 166 175 180 193 173 177 206 233 188 76 23 67 67 68 70 69 70 70 71 71 71 70 70 70 69 68 64 80 93 220 143 129 166 161 172 170 159 152 146 143 145 129 102 81 78 112 117 111 144 170 183 186 164 161 195 235 150 32 24 67 68 69 70 70 70 71 71 71 71 71 70 70 68 63 83 93 91 254 205 114 137 150 148 157 154 119 112 122 113 91 76 69 103 117 93 93 119 150 180 188 162 155 185 230 129 21 25 68 68 70 70 70 70 71 71 71 71 71 70 69 68 61 126 100 125 253 239 192 107 121 118 119 123 98 86 87 79 58 60 79 120 115 100 99 104 133 171 185 161 153 183 222 117 18 24\",\n          \"110 81 17 4 12 16 19 32 21 0 2 2 1 2 4 6 7 7 8 10 12 13 13 19 31 33 42 68 86 104 113 114 116 101 76 43 34 24 25 29 28 46 49 46 56 73 94 87 111 67 6 10 15 18 27 32 7 0 1 2 3 3 4 5 7 10 11 11 10 12 21 36 50 73 101 118 130 141 156 168 179 181 167 130 78 49 32 30 31 38 47 27 40 60 69 73 66 17 9 15 18 26 35 17 0 1 2 2 3 2 2 4 6 7 8 9 13 29 49 74 100 120 129 144 154 158 170 187 198 205 208 201 161 102 66 48 43 40 41 17 29 45 47 61 14 9 16 19 24 33 26 5 1 2 2 1 1 2 3 4 4 4 6 9 29 60 88 113 131 143 151 158 164 172 181 191 201 206 208 212 210 181 131 89 71 53 52 35 17 31 43 59 9 14 20 23 34 33 16 2 2 2 2 1 1 1 1 2 4 1 7 41 79 103 114 136 154 163 168 171 180 183 190 198 204 206 209 212 214 214 197 149 120 101 85 43 7 28 44 58 13 19 26 35 42 30 9 1 1 1 2 1 0 1 2 2 0 17 68 108 123 129 146 163 169 172 177 183 190 192 197 203 205 209 210 212 215 218 218 205 182 155 117 62 7 16 34 50 20 28 40 50 45 25 4 1 2 1 1 1 1 2 2 2 27 90 119 100 96 121 148 172 180 179 187 193 195 199 202 204 207 211 212 215 218 218 218 220 214 190 143 95 16 2 21 38 28 42 54 55 43 22 4 2 2 1 2 1 1 2 0 39 106 123 98 90 108 92 76 113 166 187 195 202 205 206 207 207 210 211 214 217 217 218 220 220 219 209 167 116 36 3 18 36 44 57 59 53 39 18 3 1 2 3 2 1 2 4 28 109 130 127 137 161 177 171 142 80 87 144 193 208 210 212 210 209 210 213 216 220 219 220 222 221 220 216 192 137 48 3 22 31 58 62 59 51 32 12 3 1 2 4 1 2 5 24 91 131 137 157 155 149 151 157 171 158 91 78 131 196 210 211 211 211 211 213 217 219 220 220 222 221 219 216 205 150 61 7 29 35 62 63 58 45 26 9 2 2 3 1 1 6 26 88 126 135 160 159 123 94 97 108 121 143 144 100 88 143 200 210 212 212 212 216 217 218 220 221 221 221 218 216 208 159 67 11 34 45 64 61 52 38 20 5 3 2 1 17 21 24 83 131 142 150 165 165 103 36 41 76 119 127 123 120 111 131 162 195 205 209 212 217 217 220 223 222 222 220 218 215 207 160 67 17 36 51 65 60 46 32 15 4 2 9 68 100 66 61 130 147 160 164 171 169 141 103 46 24 57 126 128 110 119 133 144 184 203 207 215 218 218 220 221 222 222 221 219 215 208 156 63 26 39 55 63 54 40 25 10 3 0 54 147 113 81 116 151 162 165 168 177 182 160 154 142 80 26 47 122 117 114 132 159 183 203 206 215 217 219 221 222 222 222 220 218 214 208 156 62 32 44 59 59 49 34 17 5 4 0 59 152 108 103 146 162 171 167 172 184 195 181 149 153 146 99 87 83 114 122 133 170 189 198 206 213 216 217 219 222 222 221 221 217 213 205 160 67 36 46 54 53 42 25 9 3 4 1 11 78 104 131 151 166 177 179 183 190 196 200 179 141 141 152 128 66 105 131 141 175 192 196 202 211 212 214 217 222 222 222 222 216 211 202 157 67 41 52 44 48 36 16 4 3 2 2 0 10 111 140 152 172 184 191 194 196 201 203 203 182 155 142 141 112 117 143 155 185 199 200 207 212 208 202 204 217 225 221 221 212 206 195 152 62 41 50 34 46 29 9 2 3 3 2 0 55 137 136 158 177 188 194 197 197 198 203 203 201 197 184 162 151 152 159 174 200 209 212 211 206 191 166 147 156 200 223 216 209 200 194 143 50 42 44 27 39 20 4 3 4 3 1 9 99 136 143 163 181 189 195 195 194 192 199 202 201 199 194 184 170 164 168 185 211 221 217 199 182 165 150 131 109 107 168 213 207 199 191 111 32 46 33 24 30 13 3 4 3 2 2 42 119 136 150 165 182 187 189 190 191 190 191 196 199 193 181 173 170 165 166 191 214 220 198 167 157 154 146 140 124 95 85 146 200 200 166 68 31 43 24 21 23 8 2 4 3 5 29 75 121 141 154 163 176 182 179 178 174 169 168 169 172 179 182 181 175 174 183 203 207 206 171 122 99 98 115 137 159 157 119 76 127 189 122 34 37 34 21 17 18 6 4 3 4 27 61 92 124 150 160 156 156 145 142 144 150 152 149 154 158 159 176 184 173 180 202 211 199 197 170 95 57 44 72 127 146 166 192 155 85 131 65 26 42 22 23 27 9 4 3 3 16 54 74 97 131 161 145 144 135 143 153 165 177 180 174 165 176 171 167 174 176 195 212 215 200 200 179 144 116 52 23 63 135 144 178 204 155 73 26 36 32 18 21 28 6 6 5 5 36 71 83 103 141 160 136 143 144 166 173 181 185 184 174 172 182 171 170 182 196 212 217 208 204 202 199 161 142 137 93 54 73 131 168 199 187 58 23 42 18 21 25 26 14 9 4 18 58 78 93 112 144 153 150 138 112 126 165 179 188 190 171 148 177 175 186 202 211 219 216 205 207 205 203 191 154 156 175 130 35 75 165 203 136 27 39 27 14 23 26 25 15 16 15 45 79 86 102 118 142 155 152 121 69 122 147 173 191 193 185 153 143 180 200 213 214 220 217 205 204 211 205 201 185 160 171 170 92 69 165 183 52 25 34 13 19 23 16 18 36 43 38 74 94 90 111 125 151 158 149 101 48 170 167 123 188 197 186 175 136 158 198 207 215 222 213 200 187 197 208 208 209 198 186 179 176 164 181 110 16 34 16 14 30 25 13 18 67 59 61 99 106 94 119 131 156 153 145 84 35 153 220 165 155 188 193 172 145 161 198 212 222 219 203 198 170 179 203 210 215 215 213 205 199 188 157 40 19 24 10 25 33 22 13 13 95 84 88 116 106 96 126 139 157 152 140 105 70 96 243 227 159 148 180 184 161 166 189 202 207 206 207 196 156 166 198 211 213 212 209 208 199 180 99 12 21 17 24 36 34 22 15 13 103 103 109 124 104 101 129 149 159 154 138 118 109 55 169 235 219 155 143 172 186 176 187 194 193 194 202 194 147 156 196 212 218 209 202 204 199 163 41 20 23 30 38 37 33 20 17 14 104 120 129 129 100 107 134 156 157 149 132 146 137 52 58 213 239 228 187 140 179 198 205 208 208 206 200 191 149 156 198 216 220 212 203 200 190 96 21 26 34 43 40 37 27 21 20 12 110 137 145 136 100 116 147 163 156 146 131 159 141 84 78 105 226 254 219 204 187 180 204 216 215 207 200 188 144 161 200 216 220 214 205 205 127 34 37 34 42 42 40 33 24 27 19 9 122 137 144 135 113 122 158 168 159 144 137 160 144 95 104 53 108 232 248 254 229 207 188 197 215 209 198 185 136 170 205 215 215 206 209 152 42 39 43 45 44 41 37 29 30 27 16 9 121 132 153 130 124 134 167 172 160 148 145 151 190 121 88 84 71 91 193 243 251 250 245 228 135 159 196 172 139 189 204 206 208 200 133 42 45 50 48 48 39 37 33 30 30 21 11 10 159 203 180 173 141 142 177 175 161 157 157 146 177 201 106 99 110 43 19 130 149 195 204 126 51 132 158 137 169 196 193 203 180 99 35 43 54 51 49 40 36 38 33 29 25 17 10 26 187 159 129 125 137 151 183 182 165 170 159 157 153 192 210 147 100 51 99 139 93 47 58 88 149 137 137 173 194 198 185 134 58 32 44 54 55 53 45 36 41 35 29 31 21 16 28 50 153 142 133 124 129 157 181 188 174 161 166 150 159 155 188 219 187 175 210 151 76 96 157 183 141 150 196 200 178 131 68 27 31 45 50 56 56 50 39 40 39 31 30 25 16 25 46 65 156 154 150 148 145 156 178 192 184 161 156 165 155 158 168 171 180 180 141 130 156 189 186 159 162 182 153 100 56 22 17 29 39 48 56 60 57 45 39 42 34 33 26 17 22 36 57 73 155 155 156 150 145 157 176 189 189 180 160 160 169 170 178 183 174 163 167 188 197 192 185 171 138 76 45 28 50 37 25 28 38 53 60 61 53 44 46 42 37 28 18 23 30 47 70 87 157 155 157 156 151 158 175 181 189 187 184 176 168 181 192 195 196 196 196 198 202 191 149 93 53 51 68 71 85 51 19 28 41 56 63 61 47 47 50 42 31 19 23 28 36 51 63 75 155 155 155 157 157 156 171 178 185 187 195 199 187 187 196 202 201 200 205 201 169 110 85 80 78 89 98 101 102 67 18 25 44 62 67 55 45 53 50 35 23 25 28 35 47 43 47 55 146 148 152 155 158 159 163 176 182 185 194 201 206 206 202 201 208 205 179 141 105 80 84 88 130 119 120 130 114 84 28 23 47 67 63 44 51 56 39 28 30 26 29 53 48 45 42 53 137 142 146 149 152 157 160 164 177 188 194 198 204 208 208 208 182 132 103 100 94 78 105 140 165 146 145 153 132 102 45 20 52 67 48 45 58 43 33 37 29 31 43 57 49 39 43 54 117 128 133 138 144 148 151 152 151 165 187 200 200 200 195 152 99 80 72 64 81 157 187 178 176 168 164 158 148 117 64 24 57 54 41 60 48 41 46 34 60 71 44 54 44 25 45 62 84 97 111 120 127 134 139 141 139 131 129 147 172 180 171 155 144 131 119 135 199 212 183 184 177 179 175 164 152 130 86 37 53 43 57 59 47 47 40 65 101 46 45 51 33 64 56 64 43 53 72 86 98 110 118 121 120 117 110 99 109 139 165 169 169 181 208 233 217 191 194 188 188 193 184 174 160 145 106 54 41 54 67 54 47 40 66 109 47 34 51 42 54 139 67 64 84 57 44 44 53 67 78 87 91 94 91 82 78 88 128 182 209 223 216 192 190 204 200 198 205 200 191 184 174 157 132 57 37 65 57 46 45 72 125 59 30 44 61 29 87 194 102 67 198 173 142 113 85 72 68 69 71 77 91 114 146 175 191 191 180 166 168 191 209 207 201 206 206 201 198 194 185 169 149 66 40 58 47 45 70 139 104 25 42 47 58 23 115 215 153 82\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"usage\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Training\",\n          \"PrivateTest\",\n          \"Usage\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(filename):\n",
        "    X, Y = [], []\n",
        "    first = True\n",
        "    for line in open(filename):\n",
        "        if first:\n",
        "            first = False\n",
        "        else:\n",
        "            row = line.split(',')\n",
        "            Y.append(int(row[0]))\n",
        "            X.append([int(p) for p in row[1].split()])\n",
        "\n",
        "    X, Y = np.array(X) / 255.0, np.array(Y)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "X, Y = loadData(filename)\n",
        "classes_count = len(set(Y))\n",
        "print(f\"Total de classes: {classes_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm2YZSEuFrYq",
        "outputId": "ddc61261-a7a6-417f-d0c0-ef3a3a0f4a0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de classes: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras with tensorflow backend\n",
        "N, D = X.shape\n",
        "X = X.reshape(N, 48, 48, 1) # Imagens s√£o 48x48\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividindo os dados X (imagens) e Y (r√≥tulos) em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
        "\n",
        "# Convertendo os r√≥tulos Y em one-hot encoding para treino\n",
        "y_train = (np.arange(classes_count) == y_train[:, None]).astype(np.float32)\n",
        "\n",
        "# Convertendo os r√≥tulos Y em one-hot encoding para teste\n",
        "y_test = (np.arange(classes_count) == y_test[:, None]).astype(np.float32)"
      ],
      "metadata": {
        "id": "E8eYzPATFuuS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createModel():\n",
        "    model = Sequential()\n",
        "\n",
        "    # 1¬™ camada Convolucional\n",
        "    model.add(Conv2D(64, (5, 5), input_shape=(X_train.shape[1:]), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # 2¬™ camada Convolucional\n",
        "    model.add(Conv2D(128, (5, 5),activation='relu', padding='same'))\n",
        "    model.add(Conv2D(128, (5, 5),activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # 3¬™ camada Convolucional\n",
        "    model.add(Conv2D(256, (3, 3),activation='relu', padding='same'))\n",
        "    model.add(Conv2D(256, (3, 3),activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Redes neurais totalmente conectadas\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(7))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = createModel()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swa-a9gYFzWL",
        "outputId": "1bb13e75-f1dc-42a8-b290-0fa7400802af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 48, 48, 64)        1664      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 48, 48, 64)        102464    \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 48, 48, 64)        256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 24, 24, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 24, 24, 128)       204928    \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 128)       409728    \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 24, 24, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 12, 12, 256)       1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1179776   \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation (Activation)     (None, 128)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 903       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 7)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2787015 (10.63 MB)\n",
            "Trainable params: 2785863 (10.63 MB)\n",
            "Non-trainable params: 1152 (4.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session() # Destr√≥i os gr√°ficos atuais e contr√≥i outro\n",
        "keras.backend.set_value(model.optimizer.lr, 1e-2) # Seta a taxa de aprendizado\n",
        "\n",
        "# Treinar modelo\n",
        "model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=64,\n",
        "    epochs=32,\n",
        "    verbose=1,\n",
        "    validation_data=(X_test, y_test),\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heUZSMqKF06W",
        "outputId": "92d4baf4-0f12-492a-d7e9-58cb1f9d9e42"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "505/505 [==============================] - 32s 41ms/step - loss: 1.7333 - accuracy: 0.3083 - val_loss: 1.6576 - val_accuracy: 0.3728\n",
            "Epoch 2/32\n",
            "505/505 [==============================] - 18s 35ms/step - loss: 1.4485 - accuracy: 0.4419 - val_loss: 1.4524 - val_accuracy: 0.4486\n",
            "Epoch 3/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 1.2636 - accuracy: 0.5195 - val_loss: 1.3063 - val_accuracy: 0.5010\n",
            "Epoch 4/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 1.1418 - accuracy: 0.5679 - val_loss: 1.2147 - val_accuracy: 0.5422\n",
            "Epoch 5/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 1.0407 - accuracy: 0.6086 - val_loss: 1.1152 - val_accuracy: 0.5832\n",
            "Epoch 6/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.9627 - accuracy: 0.6397 - val_loss: 1.3933 - val_accuracy: 0.5135\n",
            "Epoch 7/32\n",
            "505/505 [==============================] - 18s 35ms/step - loss: 0.8690 - accuracy: 0.6751 - val_loss: 1.2012 - val_accuracy: 0.5790\n",
            "Epoch 8/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.7709 - accuracy: 0.7127 - val_loss: 1.1637 - val_accuracy: 0.5996\n",
            "Epoch 9/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.6708 - accuracy: 0.7525 - val_loss: 1.2827 - val_accuracy: 0.5793\n",
            "Epoch 10/32\n",
            "505/505 [==============================] - 18s 35ms/step - loss: 0.5777 - accuracy: 0.7843 - val_loss: 1.2738 - val_accuracy: 0.6094\n",
            "Epoch 11/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.4856 - accuracy: 0.8242 - val_loss: 1.3705 - val_accuracy: 0.5974\n",
            "Epoch 12/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.4233 - accuracy: 0.8447 - val_loss: 1.4932 - val_accuracy: 0.5848\n",
            "Epoch 13/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.3395 - accuracy: 0.8748 - val_loss: 1.5284 - val_accuracy: 0.6133\n",
            "Epoch 14/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.3088 - accuracy: 0.8874 - val_loss: 1.7963 - val_accuracy: 0.5879\n",
            "Epoch 15/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.2604 - accuracy: 0.9077 - val_loss: 1.7287 - val_accuracy: 0.6233\n",
            "Epoch 16/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.2370 - accuracy: 0.9148 - val_loss: 1.8570 - val_accuracy: 0.6096\n",
            "Epoch 17/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.2176 - accuracy: 0.9223 - val_loss: 1.7803 - val_accuracy: 0.6138\n",
            "Epoch 18/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.2065 - accuracy: 0.9278 - val_loss: 1.8946 - val_accuracy: 0.6113\n",
            "Epoch 19/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1970 - accuracy: 0.9319 - val_loss: 1.8611 - val_accuracy: 0.5988\n",
            "Epoch 20/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1787 - accuracy: 0.9373 - val_loss: 2.0890 - val_accuracy: 0.5991\n",
            "Epoch 21/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1720 - accuracy: 0.9403 - val_loss: 2.1691 - val_accuracy: 0.6024\n",
            "Epoch 22/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1500 - accuracy: 0.9480 - val_loss: 2.0956 - val_accuracy: 0.6049\n",
            "Epoch 23/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1441 - accuracy: 0.9501 - val_loss: 2.0796 - val_accuracy: 0.6216\n",
            "Epoch 24/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1454 - accuracy: 0.9491 - val_loss: 2.0786 - val_accuracy: 0.6147\n",
            "Epoch 25/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1332 - accuracy: 0.9532 - val_loss: 2.3465 - val_accuracy: 0.6133\n",
            "Epoch 26/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1332 - accuracy: 0.9530 - val_loss: 2.3792 - val_accuracy: 0.6183\n",
            "Epoch 27/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1277 - accuracy: 0.9572 - val_loss: 2.3105 - val_accuracy: 0.6160\n",
            "Epoch 28/32\n",
            "505/505 [==============================] - 20s 39ms/step - loss: 0.1181 - accuracy: 0.9593 - val_loss: 2.3571 - val_accuracy: 0.6152\n",
            "Epoch 29/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1278 - accuracy: 0.9559 - val_loss: 2.5036 - val_accuracy: 0.6141\n",
            "Epoch 30/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1185 - accuracy: 0.9584 - val_loss: 2.2496 - val_accuracy: 0.6102\n",
            "Epoch 31/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1132 - accuracy: 0.9610 - val_loss: 2.3431 - val_accuracy: 0.6211\n",
            "Epoch 32/32\n",
            "505/505 [==============================] - 18s 36ms/step - loss: 0.1118 - accuracy: 0.9616 - val_loss: 2.1669 - val_accuracy: 0.6074\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f5de82da860>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JyPxwYSloXCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "# Salva o modelo para ser usado posteriormente\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model.to_json())\n",
        "\n",
        "model.save_weights(\"weights.h5\")"
      ],
      "metadata": {
        "id": "3kyQxnyJGOZd"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}